"""
ìŠ¤ë§ˆíŠ¸ í”¼ì²˜ ì„ íƒ ëª¨ë“ˆ - ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ë³€ìˆ˜ ì„ íƒ

í•µì‹¬ ì•„ì´ë””ì–´:
- ê¸°ìƒ ì›ë³¸ ë³€ìˆ˜ > íŒŒìƒ ë³€ìˆ˜ ìš°ì„ ìˆœìœ„
- ìƒì„±ëœ ë³€ìˆ˜ë³´ë‹¤ ê¸°ë³¸ ë³€ìˆ˜ ì„ í˜¸
"""

import pandas as pd
import numpy as np
from typing import List, Tuple, Dict, Any
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import ElasticNetCV
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')  # ëª¨ë¸ í›ˆë ¨ ì‹œ ë¶ˆí•„ìš”í•œ ê²½ê³  ì œê±°


class SmartFeatureSelector:
    """
    ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ ìŠ¤ë§ˆíŠ¸ ë³€ìˆ˜ ì„ íƒ í´ë˜ìŠ¤
    
    - Elastic Net ìë™ ë³€ìˆ˜ì„ íƒ ì¶”ê°€ (ë‹¤ì¤‘ê³µì„ ì„± í•´ê²°)
    - ê¸°ì¡´ ìƒê´€ê´€ê³„ ê¸°ë°˜ + Elastic Net ê²°í•© ì˜µì…˜ ì œê³µ
    
    **í•µì‹¬ ì² í•™:**
    - 53ê°œ ê³ ìƒê´€ ë³€ìˆ˜ìŒ â†’ Elastic Netìœ¼ë¡œ í•´ê²° ê¸°ëŒ€í•´ë´„
    - ê¸°ìƒ ì›ë³¸ ë³€ìˆ˜ëŠ” ìµœëŒ€í•œ ë³´ì¡´ (ì˜ˆì¸¡ì— ê°€ì¥ ì¤‘ìš”)
    - ë³µì¡í•œ íŒŒìƒë³€ìˆ˜ë³´ë‹¤ í•´ì„ ê°€ëŠ¥í•œ ê¸°ë³¸ ë³€ìˆ˜ ì„ í˜¸
    - ê°™ì€ ì˜ë¯¸ì˜ ì¤‘ë³µ ë³€ìˆ˜ëŠ” í‘œì¤€í™” (py_weekday â†’ weekday)
    - ëª¨ë“  ì„ íƒ ì´ìœ ë¥¼ ëª…í™•íˆ ê¸°ë¡
    
    **ìš°ì„ ìˆœìœ„ ì²´ê³„:**
    1. ê¸°ìƒ ì›ë³¸ > íŒŒìƒ ë³€ìˆ˜
    2. ê¸°ë³¸ ì‹œê°„ ë³€ìˆ˜ > ë³µì¡í•œ ìƒí˜¸ì‘ìš©
    3. ì§§ì€ ì´ë¦„ > ê¸´ ì´ë¦„ (ë‹¨ìˆœì„± ì„ í˜¸)
    """
    
    def __init__(self, correlation_threshold: float = 0.85):
        """
        ì´ˆê¸°í™”
        """
        self.correlation_threshold = correlation_threshold
        
        # ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ì²´ê³„
        # ì‘ê¸‰ì˜ë£Œ ì „ë¬¸ê°€ì™€ ê¸°ìƒ ì „ë¬¸ê°€ ì˜ê²¬ ë°˜ì˜
        self.domain_priorities = {
            'weather_original': ['ta_max', 'ta_min', 'ws_max', 'hm_max', 'hm_min', 'rn_day'],  # ê°€ì¥ ì¤‘ìš”
            'target': ['call_count'],  # íƒ€ê²Ÿ ë³€ìˆ˜
            'temporal_basic': ['year', 'month', 'day', 'weekday'],  # ê¸°ë³¸ ì‹œê°„ ë³€ìˆ˜
            'population': ['ì´ì¸êµ¬ìˆ˜', 'ì„¸ëŒ€ìˆ˜', 'ë‚¨ì.ì¸êµ¬ìˆ˜', 'ì—¬ì.ì¸êµ¬ìˆ˜'],  # ì¸êµ¬ ì •ë³´
            'location': ['xcoord', 'ycoord'],  # ìœ„ì¹˜ ì¢Œí‘œ
            'derived_important': ['temp_avg', 'humidity_avg', 'is_rainy', 'is_weekend']  # ì¤‘ìš”í•œ íŒŒìƒë³€ìˆ˜
        }
        
        # ë¡œê¹… ì‹œìŠ¤í…œ (íˆ¬ëª…ì„± í™•ë³´ìš©)
        self.removal_log = []      # ì œê±°ëœ ë³€ìˆ˜ë“¤ ê¸°ë¡
        self.selection_log = []    # ì„ íƒ ê³¼ì • ìƒì„¸ ê¸°ë¡
        self.elastic_net_log = []  # Elastic Net ì„ íƒ ê³¼ì • ê¸°ë¡
    
    def select_better_variable(self, var1: str, var2: str, correlation_val: float) -> str:
        """
        ë‘ ê³ ìƒê´€ ë³€ìˆ˜ ì¤‘ ì–´ë–¤ ê²ƒì„ ì œê±°í• ì§€ ì§€ëŠ¥ì ìœ¼ë¡œ ì„ íƒ
        

        ë„ë©”ì¸ ì§€ì‹ì— ê¸°ë°˜í•´ì„œ ë” ì¤‘ìš”í•œ ë³€ìˆ˜ë¥¼ ë³´ì¡´í•¨
        
        **ì˜ì‚¬ê²°ì • íŠ¸ë¦¬:**
        1ë‹¨ê³„: ë„ë©”ì¸ ìš°ì„ ìˆœìœ„ í™•ì¸ (ê¸°ìƒ > ì‹œê°„ > ì¸êµ¬ ë“±)
        2ë‹¨ê³„: py_weekday/weekday ê°™ì€ ì•Œë ¤ì§„ ì¤‘ë³µ ì²˜ë¦¬  
        3ë‹¨ê³„: ë³µì¡ë„ ë¹„êµ (ê¸°ë³¸ ë³€ìˆ˜ > ìƒí˜¸ì‘ìš© ë³€ìˆ˜)
        4ë‹¨ê³„: ì´ë¦„ ê¸¸ì´ (ì§§ì€ ê²ƒì´ ë³´í†µ ë” ê¸°ë³¸ì )
        5ë‹¨ê³„: ìµœí›„ ìˆ˜ë‹¨ìœ¼ë¡œ ì²« ë²ˆì§¸ ì œê±°
        
        Args:
            var1: ì²« ë²ˆì§¸ ë³€ìˆ˜ëª…
            var2: ë‘ ë²ˆì§¸ ë³€ìˆ˜ëª…
            correlation_val: ë‘ ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ê°’
            
        Returns:
            ì œê±°í•  ë³€ìˆ˜ëª… (ë³´ì¡´í•  ê²Œ ì•„ë‹ˆë¼ ì œê±°í•  ê²ƒ!)
        """
        
        # 1ë‹¨ê³„: ë„ë©”ì¸ ì§€ì‹ ìš°ì„ ìˆœìœ„ ì ìš©
        # í•œ ë³€ìˆ˜ê°€ ì¤‘ìš” ì¹´í…Œê³ ë¦¬ì— ì†í•˜ê³  ë‹¤ë¥¸ ê±´ ì•ˆ ì†í•˜ë©´ â†’ ì¤‘ìš”í•œ ê²ƒ ë³´ì¡´
        for category, variables in self.domain_priorities.items():
            if var1 in variables and var2 not in variables:
                self._log_selection(var1, var2, correlation_val, f"ë„ë©”ì¸ ìš°ì„ ìˆœìœ„: {category}")
                return var2  # var1 ë³´ì¡´, var2 ì œê±°
            elif var2 in variables and var1 not in variables:
                self._log_selection(var2, var1, correlation_val, f"ë„ë©”ì¸ ìš°ì„ ìˆœìœ„: {category}")
                return var1  # var2 ë³´ì¡´, var1 ì œê±°
        
        # 2ë‹¨ê³„: ì•Œë ¤ì§„ ì¤‘ë³µ ë³€ìˆ˜ íŠ¹ë³„ ì²˜ë¦¬
        if {var1, var2} == {'weekday', 'py_weekday'}:
            self._log_selection('weekday', 'py_weekday', correlation_val, "ì¤‘ë³µ ë³€ìˆ˜: weekday ìš°ì„ ")
            return 'py_weekday'  # py_weekday ì œê±°, weekday ë³´ì¡´
        
        # 3ë‹¨ê³„: ë³µì¡ë„ ê¸°ë°˜ ì„ íƒ
        # ë³µì¡í•œ íŒŒìƒë³€ìˆ˜(ìƒí˜¸ì‘ìš©, ë¡¤ë§ ë“±)ë³´ë‹¤ ê¸°ë³¸ ë³€ìˆ˜ ì„ í˜¸
        complex_patterns = ['interaction', 'poly', 'roll', 'lag', 'cluster', 'network']
        
        var1_complex = any(pattern in var1.lower() for pattern in complex_patterns)
        var2_complex = any(pattern in var2.lower() for pattern in complex_patterns)
        
        if var1_complex and not var2_complex:
            self._log_selection(var2, var1, correlation_val, "ë‹¨ìˆœì„± ìš°ì„ : ë³µì¡í•œ íŒŒìƒë³€ìˆ˜ ì œê±°")
            return var1  # ë³µì¡í•œ var1 ì œê±°
        elif var2_complex and not var1_complex:
            self._log_selection(var1, var2, correlation_val, "ë‹¨ìˆœì„± ìš°ì„ : ë³µì¡í•œ íŒŒìƒë³€ìˆ˜ ì œê±°")
            return var2  # ë³µì¡í•œ var2 ì œê±°
        
        # 4ë‹¨ê³„: ì´ë¦„ ê¸¸ì´ ê¸°ì¤€ (ë‹¨ìˆœì„± íœ´ë¦¬ìŠ¤í‹±)
        # ì§§ì€ ì´ë¦„ì´ ë³´í†µ ë” ê¸°ë³¸ì ì¸ ë³€ìˆ˜ (ta_max vs weather_interaction_ta_max_hm_max)
        if len(var1) != len(var2):
            if len(var1) < len(var2):
                self._log_selection(var1, var2, correlation_val, "ë‹¨ìˆœì„±: ì§§ì€ ë³€ìˆ˜ëª… ì„ í˜¸")
                return var2  # ê¸´ var2 ì œê±°
            else:
                self._log_selection(var2, var1, correlation_val, "ë‹¨ìˆœì„±: ì§§ì€ ë³€ìˆ˜ëª… ì„ í˜¸")
                return var1  # ê¸´ var1 ì œê±°
        
        # 5ë‹¨ê³„: ìµœí›„ ìˆ˜ë‹¨ (ì„ì˜ ì„ íƒ)
        # ëª¨ë“  íœ´ë¦¬ìŠ¤í‹±ì´ ì‹¤íŒ¨í•˜ë©´ ì²« ë²ˆì§¸ ë³€ìˆ˜ ì œê±°
        self._log_selection("ë¬´ì‘ìœ„", f"{var1}>{var2}", correlation_val, "ê¸°ë³¸ ê·œì¹™")
        return var1
    
    def remove_correlated_features(self, df: pd.DataFrame, 
                                 exclude_columns: List[str] = None) -> Tuple[List[str], Dict[str, Any]]:
        """
        ìƒê´€ê´€ê³„ ê¸°ë°˜ ë³€ìˆ˜ ì œê±° (ë„ë©”ì¸ ì§€ì‹ ì ìš©).
        
        Args:
            df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
            exclude_columns: ë¶„ì„ì—ì„œ ì œì™¸í•  ì»¬ëŸ¼ë“¤ (ì˜ˆ: target)
            
        Returns:
            (ìµœì¢…_ì„ íƒëœ_ë³€ìˆ˜_ë¦¬ìŠ¤íŠ¸, ìƒì„¸_ë¶„ì„_ê²°ê³¼)
        """
        
        print(f"\në„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ìƒê´€ê´€ê³„ ë¶„ì„ (ì„ê³„ê°’: {self.correlation_threshold})")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ ì„ íƒ
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        if exclude_columns:
            numeric_cols = [col for col in numeric_cols if col not in exclude_columns]
        
        print(f"   ë¶„ì„ ëŒ€ìƒ ë³€ìˆ˜: {len(numeric_cols)}ê°œ")
        
        # ìƒê´€ê´€ê³„ í–‰ë ¬ ê³„ì‚°
        corr_matrix = df[numeric_cols].corr().abs()
        
        # ìƒì‚¼ê°í–‰ë ¬ì—ì„œ ë†’ì€ ìƒê´€ê´€ê³„ ìŒ ì°¾ê¸°
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        
        to_remove = set()
        high_corr_pairs = []
        
        for i in range(len(upper.columns)):
            for j in range(len(upper.columns)):
                if pd.notna(upper.iloc[i, j]) and upper.iloc[i, j] > self.correlation_threshold:
                    var1, var2 = upper.columns[i], upper.columns[j]
                    corr_val = upper.iloc[i, j]
                    
                    high_corr_pairs.append({
                        'var1': var1,
                        'var2': var2, 
                        'correlation': corr_val
                    })
                    
                    # ë„ë©”ì¸ ì§€ì‹ìœ¼ë¡œ ì œê±°í•  ë³€ìˆ˜ ì„ íƒ
                    remove_var = self.select_better_variable(var1, var2, corr_val)
                    to_remove.add(remove_var)
        
        # ìµœì¢… ì„ íƒëœ ë³€ìˆ˜ë“¤
        final_features = [col for col in numeric_cols if col not in to_remove]
        
        analysis_result = {
            'initial_count': len(numeric_cols),
            'removed_count': len(to_remove),
            'final_count': len(final_features),
            'high_corr_pairs': high_corr_pairs,
            'removed_variables': list(to_remove),
            'final_features': final_features,
            'selection_log': self.selection_log,
            'corr_matrix': corr_matrix
        }
        
        print(f"   âœ… ì œê±°ëœ ë³€ìˆ˜: {len(to_remove)}ê°œ")
        print(f"   âœ… ìµœì¢… ë³€ìˆ˜: {len(final_features)}ê°œ")
        print(f"   ğŸ“‹ ë†’ì€ ìƒê´€ê´€ê³„ ìŒ: {len(high_corr_pairs)}ê°œ")
        
        return final_features, analysis_result
    
    def get_feature_importance_summary(self, analysis_result: Dict[str, Any]) -> pd.DataFrame:
        """ë³€ìˆ˜ ì„ íƒ ê³¼ì • ìš”ì•½ ì •ë³´ ìƒì„±"""
        
        selection_summary = []
        
        # ì„ íƒëœ ë³€ìˆ˜ë“¤ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
        for feature in analysis_result['final_features']:
            category = self._classify_feature(feature)
            
            selection_summary.append({
                'feature_name': feature,
                'category': category,
                'selection_reason': 'passed_correlation_filter',
                'domain_priority': self._get_domain_priority(feature),
                'is_original_weather': feature in self.domain_priorities['weather_original'],
                'is_derived': self._is_derived_feature(feature)
            })
        
        return pd.DataFrame(selection_summary)
    
    def _log_selection(self, kept_var: str, removed_var: str, 
                      correlation: float, reason: str):
        """ë³€ìˆ˜ ì„ íƒ ê³¼ì • ë¡œê¹…"""
        self.selection_log.append({
            'kept_variable': kept_var,
            'removed_variable': removed_var,
            'correlation': correlation,
            'selection_reason': reason,
            'timestamp': pd.Timestamp.now()
        })
    
    def _classify_feature(self, feature: str) -> str:
        """ë³€ìˆ˜ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        for category, variables in self.domain_priorities.items():
            if feature in variables:
                return category
        
        # íŒ¨í„´ ê¸°ë°˜ ë¶„ë¥˜
        if any(pattern in feature.lower() for pattern in ['interaction', 'poly']):
            return 'interaction_features'
        elif any(pattern in feature.lower() for pattern in ['roll', 'lag']):
            return 'temporal_features'
        elif any(pattern in feature.lower() for pattern in ['cluster', 'network']):
            return 'spatial_features'
        elif feature.startswith('pca_'):
            return 'pca_features'
        else:
            return 'other_features'
    
    def _get_domain_priority(self, feature: str) -> int:
        """ë³€ìˆ˜ì˜ ë„ë©”ì¸ ìš°ì„ ìˆœìœ„ ë°˜í™˜ (1=ìµœê³ )"""
        priority_order = [
            'weather_original', 'target', 'temporal_basic', 
            'derived_important', 'population', 'location'
        ]
        
        for i, category in enumerate(priority_order):
            if feature in self.domain_priorities.get(category, []):
                return i + 1
        
        return 10  # ê¸°íƒ€
    
    def _is_derived_feature(self, feature: str) -> bool:
        """íŒŒìƒ ë³€ìˆ˜ ì—¬ë¶€ í™•ì¸"""
        derived_patterns = [
            'avg', 'range', 'interaction', 'poly', 'roll', 'lag', 
            'cluster', 'network', 'pca_', 'sin', 'cos'
        ]
        return any(pattern in feature.lower() for pattern in derived_patterns)
    
    def save_analysis_results(self, analysis_result: Dict[str, Any], 
                            output_prefix: str = "smart_selection"):
        """ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        
        # outputs í´ë” ìƒì„±
        import os
        os.makedirs('outputs', exist_ok=True)
        
        # Elastic Net ê²°ê³¼ì¸ì§€ í™•ì¸
        is_elastic_net = 'feature_coefficients' in analysis_result
        
        if is_elastic_net:
            # Elastic Net ê²°ê³¼ ì €ì¥
            print(f"\nElastic Net ë¶„ì„ ê²°ê³¼ ì €ì¥:")
            
            # 1. ì„ íƒëœ ë³€ìˆ˜ì™€ ê³„ìˆ˜
            coef_data = []
            for feature, coef in analysis_result['feature_coefficients'].items():
                coef_data.append({
                    'feature': feature,
                    'coefficient': coef,
                    'abs_coefficient': abs(coef),
                    'selected': abs(coef) > 1e-8
                })
            
            coef_df = pd.DataFrame(coef_data)
            coef_df = coef_df.sort_values('abs_coefficient', ascending=False)
            coef_df.to_csv(f'outputs/{output_prefix}_coefficients.csv', 
                          index=False, encoding='utf-8')
            
            # 2. ìµœì  íŒŒë¼ë¯¸í„° ì •ë³´
            params_data = {
                'parameter': ['best_alpha', 'best_l1_ratio', 'cv_r2_score', 'selected_features_count'],
                'value': [
                    analysis_result['best_alpha'],
                    analysis_result['best_l1_ratio'], 
                    analysis_result['cv_score'],
                    analysis_result['selected_count']
                ]
            }
            params_df = pd.DataFrame(params_data)
            params_df.to_csv(f'outputs/{output_prefix}_parameters.csv', 
                            index=False, encoding='utf-8')
            
            # 3. ì„ íƒëœ ë³€ìˆ˜ ëª©ë¡
            selected_df = pd.DataFrame({'selected_features': analysis_result['selected_features']})
            selected_df.to_csv(f'outputs/{output_prefix}_selected_features.csv', 
                              index=False, encoding='utf-8')
            
            print(f"   - outputs/{output_prefix}_coefficients.csv")
            print(f"   - outputs/{output_prefix}_parameters.csv")
            print(f"   - outputs/{output_prefix}_selected_features.csv")
            
            return coef_df
            
        else:
            # ê¸°ì¡´ ìƒê´€ê´€ê³„ ê¸°ë°˜ ê²°ê³¼ ì €ì¥
            # 1. ë†’ì€ ìƒê´€ê´€ê³„ ìŒ
            if analysis_result.get('high_corr_pairs'):
                corr_df = pd.DataFrame(analysis_result['high_corr_pairs'])
                corr_df = corr_df.sort_values('correlation', ascending=False)
                corr_df.to_csv(f'outputs/{output_prefix}_high_correlations.csv', 
                              index=False, encoding='utf-8')
            
            # 2. ì œê±°ëœ ë³€ìˆ˜ ìƒì„¸ ì •ë³´
            if self.selection_log:
                selection_df = pd.DataFrame(self.selection_log)
                selection_df.to_csv(f'outputs/{output_prefix}_selection_log.csv', 
                                   index=False, encoding='utf-8')
            
            # 3. ìµœì¢… ì„ íƒëœ ë³€ìˆ˜ ìš”ì•½
            feature_summary = self.get_feature_importance_summary(analysis_result)
            feature_summary.to_csv(f'outputs/{output_prefix}_final_features.csv', 
                                  index=False, encoding='utf-8')
            
            # 4. ìƒê´€ê´€ê³„ í–‰ë ¬
            if 'corr_matrix' in analysis_result:
                analysis_result['corr_matrix'].to_csv(f'outputs/{output_prefix}_correlation_matrix.csv', 
                                                     encoding='utf-8')
            
            print(f"\nğŸ’¾ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ ì €ì¥:")
            print(f"   - outputs/{output_prefix}_high_correlations.csv")
            print(f"   - outputs/{output_prefix}_selection_log.csv")  
            print(f"   - outputs/{output_prefix}_final_features.csv")
            print(f"   - outputs/{output_prefix}_correlation_matrix.csv")
            
            return feature_summary
    
    def validate_selection(self, train_df: pd.DataFrame, test_df: pd.DataFrame,
                          final_features: List[str], target_col: str = 'call_count') -> Dict[str, float]:
        """ì„ íƒëœ ë³€ìˆ˜ë“¤ë¡œ ê°„ë‹¨í•œ ì„±ëŠ¥ ê²€ì¦"""
        
        from sklearn.ensemble import RandomForestRegressor
        from lightgbm import LGBMRegressor
        
        X_train = train_df[final_features].fillna(0)
        X_test = test_df[final_features].fillna(0)
        y_train = train_df[target_col]
        y_test = test_df[target_col]
        
        results = {}
        
        # RandomForestë¡œ ê°„ë‹¨ ê²€ì¦
        rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        rf_model.fit(X_train, y_train)
        rf_pred = rf_model.predict(X_test)
        
        results['RandomForest'] = {
            'RMSE': np.sqrt(mean_squared_error(y_test, rf_pred)),
            'R2': r2_score(y_test, rf_pred)
        }
        
        # LightGBMìœ¼ë¡œ ê°„ë‹¨ ê²€ì¦
        lgbm_model = LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)
        lgbm_model.fit(X_train, y_train)
        lgbm_pred = lgbm_model.predict(X_test)
        
        results['LightGBM'] = {
            'RMSE': np.sqrt(mean_squared_error(y_test, lgbm_pred)),
            'R2': r2_score(y_test, lgbm_pred)
        }
        
        print(f"\nğŸ¯ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì„ íƒ ë³€ìˆ˜ ì„±ëŠ¥ ê²€ì¦:")
        print(f"   RF:  RMSE {results['RandomForest']['RMSE']:.4f}, RÂ² {results['RandomForest']['R2']:.4f}")
        print(f"   LGBM: RMSE {results['LightGBM']['RMSE']:.4f}, RÂ² {results['LightGBM']['R2']:.4f}")
        
        return results
    
    def elastic_net_selection(self, train_df: pd.DataFrame, target_col: str = 'call_count',
                             cv_folds: int = 5, l1_ratio_range: List[float] = None,
                             alpha_range: List[float] = None) -> Tuple[List[str], Dict[str, Any]]:
        """
        **í•µì‹¬ ì•„ì´ë””ì–´:**
        - L1 ì •ê·œí™”: ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ìë™ ì œê±°
        - L2 ì •ê·œí™”: ìƒê´€ê´€ê³„ ë†’ì€ ë³€ìˆ˜ë“¤ì„ ê·¸ë£¹ìœ¼ë¡œ ì²˜ë¦¬
        - Cross-validationìœ¼ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ìë™ ì„ íƒ
        - ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°
        
        Args:
            train_df: í›ˆë ¨ ë°ì´í„°
            target_col: íƒ€ê²Ÿ ë³€ìˆ˜ëª…
            cv_folds: Cross-validation fold ìˆ˜
            l1_ratio_range: L1/L2 ë¹„ìœ¨ ë²”ìœ„ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            alpha_range: ì •ê·œí™” ê°•ë„ ë²”ìœ„ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            
        Returns:
            (ì„ íƒëœ_ë³€ìˆ˜_ë¦¬ìŠ¤íŠ¸, ìƒì„¸_ë¶„ì„_ê²°ê³¼)
        """
        
        print(f"\nElastic Net ìë™ ë³€ìˆ˜ì„ íƒ ì‹œì‘")
        print(f"   ë‹¤ì¤‘ê³µì„ ì„± í•´ê²°ì„ ìœ„í•œ L1+L2 ì •ê·œí™” ì ìš©")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ ì„ íƒ (íƒ€ê²Ÿ ì œì™¸)
        numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
        if target_col in numeric_cols:
            numeric_cols.remove(target_col)
        
        print(f"   ë¶„ì„ ëŒ€ìƒ ë³€ìˆ˜: {len(numeric_cols)}ê°œ")
        
        # ë°ì´í„° ì¤€ë¹„
        X = train_df[numeric_cols].fillna(0)
        y = train_df[target_col]
        
        # ë¬´í•œëŒ€ì™€ ë§¤ìš° í° ê°’ ì²˜ë¦¬ (StandardScaler ì—ëŸ¬ ë°©ì§€)
        print(f"   ë°ì´í„° ì „ì²˜ë¦¬: inf/nan ê°’ ì²˜ë¦¬ ì¤‘...")
        
        # ëª¨ë“  ì»¬ëŸ¼ì„ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ê°•ì œ ë³€í™˜ (object íƒ€ì… ë¬¸ì œ í•´ê²°)
        for col in X.columns:
            try:
                X[col] = pd.to_numeric(X[col], errors='coerce')
            except:
                print(f"   âš ï¸  {col} ì»¬ëŸ¼ ìˆ˜ì¹˜í˜• ë³€í™˜ ì‹¤íŒ¨ - 0ìœ¼ë¡œ ëŒ€ì²´")
                X[col] = 0
        
        # inf ê°’ì„ nanìœ¼ë¡œ ë³€í™˜ í›„ 0ìœ¼ë¡œ ëŒ€ì²´
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(0)
        
        # ë§¤ìš° í° ê°’ë“¤ì„ í´ë¦¬í•‘ (overflow ë°©ì§€)
        # ê° ì»¬ëŸ¼ë³„ë¡œ 99.9% ë¶„ìœ„ìˆ˜ë¥¼ ìƒí•œìœ¼ë¡œ ì„¤ì •
        for col in X.columns:
            try:
                if X[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                    upper_bound = X[col].quantile(0.999)
                    lower_bound = X[col].quantile(0.001)
                    X[col] = X[col].clip(lower=lower_bound, upper=upper_bound)
            except:
                print(f"   âš ï¸  {col} ì»¬ëŸ¼ í´ë¦¬í•‘ ì‹¤íŒ¨ - ê±´ë„ˆëœ€")
                continue
        
        # ìµœì¢… í™•ì¸ (ì•ˆì „í•œ ë°©ë²•ìœ¼ë¡œ)
        try:
            # DataFrame ë‹¨ìœ„ë¡œ í™•ì¸
            inf_count = X.isin([np.inf, -np.inf]).sum().sum()
            nan_count = X.isna().sum().sum()
            print(f"   ì „ì²˜ë¦¬ í›„ inf: {inf_count}ê°œ, nan: {nan_count}ê°œ")
            
            if inf_count > 0 or nan_count > 0:
                print(f"   âš ï¸  ì—¬ì „íˆ inf/nan ì¡´ì¬ - 0ìœ¼ë¡œ ìµœì¢… ëŒ€ì²´")
                X = X.fillna(0)
                X = X.replace([np.inf, -np.inf], 0)
        except Exception as e:
            print(f"   âš ï¸  inf/nan í™•ì¸ ì¤‘ ì—ëŸ¬: {e}")
            print(f"   ì•ˆì „ì„ ìœ„í•´ ëª¨ë“  ë¹„ì •ìƒê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´")
            X = X.fillna(0)
            X = X.replace([np.inf, -np.inf], 0)
        
        # ë°ì´í„° íƒ€ì… ìµœì¢… í™•ì¸
        print(f"   ë°ì´í„° íƒ€ì… í™•ì¸: {X.dtypes.value_counts().to_dict()}")
        
        # í‘œì¤€í™” (Elastic Netì€ ìŠ¤ì¼€ì¼ì— ë¯¼ê°)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì •
        if l1_ratio_range is None:
            l1_ratio_range = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99]  # L1 ë¹„ì¤‘
        
        if alpha_range is None:
            # ìë™ìœ¼ë¡œ ì ì ˆí•œ alpha ë²”ìœ„ ìƒì„± (50ê°œ â†’ 20ê°œë¡œ ì¶•ì†Œ)
            alpha_range = np.logspace(-3, 0, 20)  # 0.001 ~ 1 ë²”ìœ„
        
        print(f"   L1 ratio í›„ë³´: {len(l1_ratio_range)}ê°œ")
        print(f"   Alpha í›„ë³´: {len(alpha_range)}ê°œ")
        print(f"   ì´ ì¡°í•©: {len(l1_ratio_range) * len(alpha_range)}ê°œ (CV ì ìš©ì‹œ {len(l1_ratio_range) * len(alpha_range) * cv_folds}íšŒ í›ˆë ¨)")
        
        # Elastic Net Cross-Validation
        elastic_net = ElasticNetCV(
            l1_ratio=l1_ratio_range,
            alphas=alpha_range,
            cv=cv_folds,
            random_state=42,
            max_iter=2000,
            n_jobs=-1
        )
        
        print(f"   ğŸ”„ {cv_folds}-fold CVë¡œ ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰ ì¤‘...")
        
        elastic_net.fit(X_scaled, y)
        
        # ê²°ê³¼ ë¶„ì„
        selected_features = []
        feature_coefficients = {}
        
        for i, coef in enumerate(elastic_net.coef_):
            feature_name = numeric_cols[i]
            feature_coefficients[feature_name] = coef
            
            if abs(coef) > 1e-8:  # 0ì´ ì•„ë‹Œ ê³„ìˆ˜ë§Œ ì„ íƒ
                selected_features.append(feature_name)
                self.elastic_net_log.append({
                    'feature': feature_name,
                    'coefficient': coef,
                    'abs_coefficient': abs(coef),
                    'selected': True
                })
            else:
                self.elastic_net_log.append({
                    'feature': feature_name,
                    'coefficient': coef,
                    'abs_coefficient': abs(coef),
                    'selected': False
                })
        
        # ê²°ê³¼ ì •ë¦¬
        analysis_result = {
            'initial_count': len(numeric_cols),
            'selected_count': len(selected_features),
            'removed_count': len(numeric_cols) - len(selected_features),
            'selected_features': selected_features,
            'feature_coefficients': feature_coefficients,
            'best_alpha': elastic_net.alpha_,
            'best_l1_ratio': elastic_net.l1_ratio_,
            'cv_score': elastic_net.score(X_scaled, y),
            'elastic_net_model': elastic_net,
            'scaler': scaler,
            'selection_log': self.elastic_net_log
        }
        
        print(f"\n   âœ… Elastic Net ì„ íƒ ì™„ë£Œ:")
        print(f"      ìµœì  Alpha: {elastic_net.alpha_:.6f}")
        print(f"      ìµœì  L1 ratio: {elastic_net.l1_ratio_:.3f}")
        print(f"      CV RÂ² ì ìˆ˜: {elastic_net.score(X_scaled, y):.4f}")
        print(f"      ì„ íƒëœ ë³€ìˆ˜: {len(selected_features)}ê°œ (ì œê±°: {len(numeric_cols) - len(selected_features)}ê°œ)")
        
        # ì¤‘ìš”í•œ ë³€ìˆ˜ top 10 ì¶œë ¥
        sorted_features = sorted(
            [(name, abs(coef)) for name, coef in feature_coefficients.items() if abs(coef) > 1e-8],
            key=lambda x: x[1], reverse=True
        )
        
        print(f"\n   ğŸ† ì¤‘ìš” ë³€ìˆ˜ Top 10:")
        for i, (feature, abs_coef) in enumerate(sorted_features[:10]):
            print(f"      {i+1:2d}. {feature:<30} (ê³„ìˆ˜: {abs_coef:.4f})")
        
        return selected_features, analysis_result
    
    def combined_selection(self, train_df: pd.DataFrame, target_col: str = 'call_count',
                          use_elastic_net: bool = True, use_correlation: bool = True) -> Tuple[List[str], Dict[str, Any]]:
        """
        ì—˜ë¼ìŠ¤í‹±ë„· + ê¸°ì¡´ ë°©ë²• ê²°í•© ì„ íƒ
        
        **ì „ëµ:**
        1ë‹¨ê³„: Elastic Netìœ¼ë¡œ ëŒ€ëŸ‰ ë³€ìˆ˜ì„ íƒ (ë‹¤ì¤‘ê³µì„ ì„± í•´ê²°)
        2ë‹¨ê³„: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ìƒê´€ê´€ê³„ ì •ì œ (ì„ íƒì )
        3ë‹¨ê³„: ìµœì¢… ê²€ì¦ ë° ë³´ê³ ì„œ ìƒì„±
        
        Args:
            train_df: í›ˆë ¨ ë°ì´í„°
            target_col: íƒ€ê²Ÿ ë³€ìˆ˜
            use_elastic_net: Elastic Net ì‚¬ìš© ì—¬ë¶€
            use_correlation: ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¶”ê°€ ì •ì œ ì—¬ë¶€
            
        Returns:
            (ìµœì¢…_ì„ íƒëœ_ë³€ìˆ˜_ë¦¬ìŠ¤íŠ¸, ì „ì²´_ë¶„ì„_ê²°ê³¼)
        """
        
        print(f"\ní†µí•© ë³€ìˆ˜ì„ íƒ ì‹œì‘")
        print(f"   Elastic Net: {'âœ…' if use_elastic_net else 'âŒ'}")
        print(f"   ìƒê´€ê´€ê³„ ì •ì œ: {'âœ…' if use_correlation else 'âŒ'}")
        
        combined_result = {
            'elastic_net_results': None,
            'correlation_results': None,
            'final_features': [],
            'selection_steps': []
        }
        
        current_features = None
        current_df = train_df.copy()
        
        # 1ë‹¨ê³„: Elastic Net ì„ íƒ
        if use_elastic_net:
            print(f"\n=== 1ë‹¨ê³„: Elastic Net ìë™ ë³€ìˆ˜ì„ íƒ ===")
            selected_features, elastic_result = self.elastic_net_selection(
                current_df, target_col
            )
            
            combined_result['elastic_net_results'] = elastic_result
            combined_result['selection_steps'].append({
                'step': 'elastic_net',
                'input_features': len(current_df.select_dtypes(include=[np.number]).columns) - 1,
                'output_features': len(selected_features),
                'method': 'L1+L2 regularization'
            })
            
            current_features = selected_features + [target_col]  # íƒ€ê²Ÿ í¬í•¨
            current_df = current_df[current_features]
            
            print(f"   Elastic Net í›„ ë³€ìˆ˜ ìˆ˜: {len(selected_features)}ê°œ")
        
        # 2ë‹¨ê³„: ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¶”ê°€ ì •ì œ (ì„ íƒì )
        if use_correlation and current_features:
            print(f"\n=== 2ë‹¨ê³„: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ìƒê´€ê´€ê³„ ì •ì œ ===")
            final_features, correlation_result = self.remove_correlated_features(
                current_df, exclude_columns=[target_col]
            )
            
            combined_result['correlation_results'] = correlation_result
            combined_result['selection_steps'].append({
                'step': 'correlation_refinement',
                'input_features': len(selected_features) if use_elastic_net else len(current_df.columns) - 1,
                'output_features': len(final_features),
                'method': 'domain_knowledge_correlation'
            })
            
        else:
            final_features = current_features[:-1] if current_features else []  # íƒ€ê²Ÿ ì œì™¸
        
        combined_result['final_features'] = final_features
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š í†µí•© ì„ íƒ ê²°ê³¼ ìš”ì•½:")
        print(f"   ì›ë³¸ ë³€ìˆ˜ ìˆ˜: {len(train_df.select_dtypes(include=[np.number]).columns) - 1}ê°œ")
        
        for step in combined_result['selection_steps']:
            print(f"   {step['step']}: {step['input_features']} â†’ {step['output_features']}ê°œ ({step['method']})")
        
        print(f"   ìµœì¢… ì„ íƒ: {len(final_features)}ê°œ ë³€ìˆ˜")
        
        # ë³€ìˆ˜ ì œê±°ìœ¨ ê³„ì‚°
        original_count = len(train_df.select_dtypes(include=[np.number]).columns) - 1
        removal_rate = (original_count - len(final_features)) / original_count * 100
        print(f"   ë³€ìˆ˜ ì œê±°ìœ¨: {removal_rate:.1f}%")
        
        return final_features, combined_result 